{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834c44be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Franco\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707627d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "MODEL_CHECKPOINT = \"C:\\\\Users\\\\Franco\\\\Desktop\\\\projetos\\\\bert\\\\bert-large-portuguese-cased\"\n",
    "DATASET_PATH = \"./data/train_mlm.csv\"\n",
    "TEXT_COLUMN = \"review\"\n",
    "\n",
    "OUTPUT_BASE = \"./bertimbau_dapt_runs\"\n",
    "CHUNK_SIZE = 512\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0416036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 4828\n",
      "Unique reviews: 1322\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train2024.csv\", sep=\";\")\n",
    "\n",
    "# mantÃ©m uma ocorrÃªncia por review\n",
    "df_mlm = df[[TEXT_COLUMN]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Unique reviews: {len(df_mlm)}\")\n",
    "\n",
    "df_mlm.to_csv(DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b91775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1322 examples [00:00, 87914.73 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1322/1322 [00:00<00:00, 8811.51 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "dataset_full = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=DATASET_PATH,\n",
    "    sep=\";\"\n",
    ")[\"train\"]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        truncation=True,\n",
    "        max_length=CHUNK_SIZE,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized = dataset_full.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_full.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# fiz um teste com e sem agrupar para usar ao mÃ¡ximo a capacidade do modelo\n",
    "# vi um artigo recomendando fazer isso\n",
    "# ficou tudo duplicado pra testar isso\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "\n",
    "    total_length = (total_length // CHUNK_SIZE) * CHUNK_SIZE\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def sample_config():\n",
    "    return {\n",
    "        \"lr\": 10 ** np.random.uniform(-5.1, -4.3), \n",
    "        \"mlm_prob\": random.choice([0.15, 0.15, 0.20]), # Peso maior para 0.15\n",
    "        \"warmup\": np.random.uniform(0.05, 0.10),\n",
    "        \"wd\": random.choice([0.01, 0.1]),\n",
    "    }\n",
    "\n",
    "N_RUNS = 50\n",
    "N_SPLITS = 5\n",
    "\n",
    "configs = [sample_config() for _ in range(N_RUNS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367cfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66012506",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = []\n",
    "\n",
    "# random search com k-fold cross validation\n",
    "\n",
    "for config_idx, cfg in enumerate(configs):\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"Config {config_idx+1}/{len(configs)}: LR={cfg['lr']} | MLM={cfg['mlm_prob']}\")\n",
    "    print(f\"==================================================\")\n",
    "    fold_ppls = []\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(tokenized)):\n",
    "        run_name = f\"conf{config_idx}_fold{fold}\"\n",
    "        output_dir = Path(OUTPUT_BASE) / f\"cfg_{config_idx}\" / f\"fold_{fold}\"\n",
    "        \n",
    "        train_sub = tokenized.select(train_idx)\n",
    "        val_sub = tokenized.select(val_idx)\n",
    "        \n",
    "        train_grouped = train_sub.map(group_texts, batched=True, desc=f\"Grouping Train F{fold+1}\")\n",
    "        val_grouped = val_sub.map(group_texts, batched=True, desc=f\"Grouping Val F{fold+1}\")\n",
    "\n",
    "        model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=cfg[\"mlm_prob\"]\n",
    "        )\n",
    "\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            overwrite_output_dir=True,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_steps=100,\n",
    "            learning_rate=cfg[\"lr\"],\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=4,\n",
    "            weight_decay=cfg[\"wd\"],\n",
    "            warmup_ratio=cfg[\"warmup\"],\n",
    "            fp16=True,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_grouped,\n",
    "            eval_dataset=val_grouped,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        eval_metrics = trainer.evaluate()\n",
    "        ppl = math.exp(eval_metrics[\"eval_loss\"])\n",
    "        fold_ppls.append(ppl)\n",
    "        \n",
    "        print(f\"   ---> Fold {fold+1}/{N_SPLITS} PPL: {ppl:.2f}\")\n",
    "\n",
    "        del model, trainer, train_grouped, val_grouped\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # mÃ©dia da perplexidade dos folds\n",
    "    avg_ppl = np.mean(fold_ppls)\n",
    "    std_ppl = np.std(fold_ppls)\n",
    "\n",
    "    print(f\"Config Result: Mean PPL = {avg_ppl:.2f} (+/- {std_ppl:.2f})\")\n",
    "\n",
    "    grouped_results.append({\n",
    "        \"learning_rate\": cfg[\"lr\"],\n",
    "        \"mlm_probability\": cfg[\"mlm_prob\"],\n",
    "        \"warmup\": cfg[\"warmup\"],\n",
    "        \"weight_decay\": cfg[\"wd\"],\n",
    "        \"perplexity_mean\": avg_ppl,\n",
    "        \"perplexity_std\": std_ppl,\n",
    "        \"folds_ppl\": fold_ppls\n",
    "    })\n",
    "\n",
    "print(\"\\nFINAL RESULTS (Sorted by Mean Perplexity)\")\n",
    "# Ordena pelo menor erro mÃ©dio (Perplexidade mÃ©dia)\n",
    "for r in sorted(grouped_results, key=lambda x: x[\"perplexity_mean\"]):\n",
    "    print(f\"PPL: {r['perplexity_mean']:.2f} | LR: {r['learning_rate']} | MLM: {r['mlm_probability']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_grouped_results = []\n",
    "\n",
    "for config_idx, cfg in enumerate(configs):\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"Config {config_idx+1}/{len(configs)}: LR={cfg['lr']} | MLM={cfg['mlm_prob']}\")\n",
    "    print(f\"==================================================\")\n",
    "\n",
    "    fold_ppls = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(tokenized)):\n",
    "        \n",
    "        run_name = f\"conf{config_idx}_lr{cfg['lr']}_fold{fold}_nongrouped\"\n",
    "        output_dir = Path(OUTPUT_BASE) / f\"cfg_{config_idx}\" / f\"fold_{fold}\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\nðŸš€ Running Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "        train_sub = tokenized.select(train_idx)\n",
    "        val_sub = tokenized.select(val_idx)\n",
    "\n",
    "        model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=cfg[\"mlm_prob\"]\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            overwrite_output_dir=True,\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_steps=100,\n",
    "            save_strategy=\"no\",\n",
    "            learning_rate=cfg[\"lr\"],\n",
    "            num_train_epochs=2, # treinamento rÃ¡pido para o Grid Search\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=4,\n",
    "            weight_decay=cfg[\"wd\"],\n",
    "            warmup_ratio=cfg[\"warmup\"],\n",
    "            fp16=True,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_sub,\n",
    "            eval_dataset=val_sub,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "        ppl = math.exp(eval_results[\"eval_loss\"])\n",
    "        fold_ppls.append(ppl)\n",
    "\n",
    "        print(f\"Fold {fold+1} Perplexity: {ppl:.2f}\")\n",
    "\n",
    "        del model, trainer, train_sub, val_sub\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_ppl = np.mean(fold_ppls)\n",
    "    std_ppl = np.std(fold_ppls)\n",
    "\n",
    "    print(f\"ðŸ“Š Config Result: Mean PPL = {avg_ppl:.2f} (+/- {std_ppl:.2f})\")\n",
    "\n",
    "    non_grouped_results.append({\n",
    "        \"learning_rate\": cfg[\"lr\"],\n",
    "        \"mlm_probability\": cfg[\"mlm_prob\"],\n",
    "        \"warmup\": cfg[\"warmup\"],\n",
    "        \"weight_decay\": cfg[\"wd\"],\n",
    "        \"grouped\": False,\n",
    "        \"perplexity_mean\": avg_ppl,\n",
    "        \"perplexity_std\": std_ppl,\n",
    "        \"folds_ppl\": fold_ppls\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\nFINAL RESULTS (Sorted by Mean Perplexity)\")\n",
    "for r in sorted(non_grouped_results, key=lambda x: x[\"perplexity_mean\"]):\n",
    "    print(f\"PPL: {r['perplexity_mean']:.2f} | LR: {r['learning_rate']} | MLM: {r['mlm_probability']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7490323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_file = Path(OUTPUT_BASE) / \"dapt_results_grouped.txt\"\n",
    "\n",
    "with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Domain-Adaptive Pretraining Results (MLM)\\n\")\n",
    "    f.write(f\"Date: {datetime.now()}\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    for r in sorted(grouped_results, key=lambda x: x[\"perplexity_mean\"]):\n",
    "        f.write(\n",
    "            f\"PPL: {r['perplexity_mean']:.4f}\\t\"\n",
    "            f\"LR: {r['learning_rate']}\\t\"\n",
    "            f\"MLM_PROB: {r['mlm_probability']}\\t\"\n",
    "            f\"WARMUP: {r['warmup']}\\t\"\n",
    "            f\"WD: {r['weight_decay']}\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceab72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = Path(OUTPUT_BASE) / \"dapt_results_non_grouped.txt\"\n",
    "\n",
    "with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Domain-Adaptive Pretraining Results (MLM)\\n\")\n",
    "    f.write(f\"Date: {datetime.now()}\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    for r in sorted(non_grouped_results, key=lambda x: x[\"perplexity_mean\"]):\n",
    "        f.write(\n",
    "            f\"PPL: {r['perplexity_mean']:.4f}\\t\"\n",
    "            f\"LR: {r['learning_rate']}\\t\"\n",
    "            f\"MLM_PROB: {r['mlm_probability']}\\t\"\n",
    "            f\"WARMUP: {r['warmup']}\\t\"\n",
    "            f\"WD: {r['weight_decay']}\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760dfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_config = {\n",
    "    \"learning_rate\": 4.704415004453045e-05,\n",
    "    \"mlm_probability\": 0.15,\n",
    "    \"warmup\": 0.08,\n",
    "    \"weight_decay\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.335, 'grad_norm': 5.9935526847839355, 'learning_rate': 1.2802178536708286e-05, 'epoch': 3.012084592145015}\n",
      "{'train_runtime': 345.6173, 'train_samples_per_second': 15.3, 'train_steps_per_second': 0.961, 'train_loss': 1.2970994696559677, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Treinamento final com dados nÃ£o agrupados\n",
    "FINAL_OUTPUT = \"./bertimbau_dapt_final_nongrouped\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=final_config[\"mlm_probability\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FINAL_OUTPUT,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=final_config[\"learning_rate\"],\n",
    "    weight_decay=final_config[\"weight_decay\"],\n",
    "    warmup_ratio=final_config[\"warmup\"],\n",
    "    fp16=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,  # pode juntar train+val aqui\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(FINAL_OUTPUT)\n",
    "tokenizer.save_pretrained(FINAL_OUTPUT)\n",
    "\n",
    "\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ddf00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping Full Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1322/1322 [00:00<00:00, 2221.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 245.3238, 'train_samples_per_second': 4.207, 'train_steps_per_second': 0.277, 'train_loss': 1.5627224866081686, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Treinamento final com dados agrupados\n",
    "FINAL_OUTPUT = \"./bertimbau_dapt_final_grouped\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "grouped_dataset = tokenized.map(group_texts, batched=True, desc=\"Grouping Full Dataset\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=final_config[\"mlm_probability\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FINAL_OUTPUT,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=final_config[\"learning_rate\"],\n",
    "    weight_decay=final_config[\"weight_decay\"],\n",
    "    warmup_ratio=final_config[\"warmup\"],\n",
    "    fp16=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=grouped_dataset,  # pode juntar train+val aqui\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(FINAL_OUTPUT)\n",
    "tokenizer.save_pretrained(FINAL_OUTPUT)\n",
    "\n",
    "\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
