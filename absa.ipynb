{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33c65d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import evaluate\n",
    "import ast\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dab54275",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4a711ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\\\Users\\\\Franco\\\\Desktop\\\\projetos\\\\bert\"\n",
    "# BERT_MODEL_PATH = f\"{PATH}\\\\bert-large-portuguese-cased\"\n",
    "BERT_MODEL_PATH = f\"{PATH}\\\\bertimbau_dapt_final_nongrouped\"\n",
    "# BERT_MODEL_PATH = f\"{PATH}\\\\bertimbau_dapt_final_grouped\"\n",
    "MODEL_NAME = \"models/absa_nlstm_ncrf_3concat.pt\"\n",
    "\n",
    "# a grande maioria dos reviews não passam de 300 tokens, então isso agiliza o treinamento\n",
    "MAX_LENGTH = 300\n",
    "K = 5\n",
    "NUM_EPOCHS = 4\n",
    "USE_LSTM = False\n",
    "USE_CRF = False\n",
    "CONCAT_LAST_N_BERT_LAYERS = 3\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "RUN_CROSS_VALIDATION = False\n",
    "LSTM_HIDDEN = 256\n",
    "DROPOUT = 0.3\n",
    "BERT_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f4aadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSA_LSTM_CRF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name,\n",
    "        num_labels=7,\n",
    "        bert_dropout=0.1,\n",
    "        dropout=0.3,\n",
    "        use_lstm=True,\n",
    "        use_crf=True,\n",
    "        concat_last_n_bert_layers=1,\n",
    "        lstm_hidden=256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_lstm = use_lstm\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # 1. BERT\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            bert_model_name,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        # Dropout menor para BERT\n",
    "        self.bert_dropout = nn.Dropout(bert_dropout)\n",
    "\n",
    "        # Dropout maior para outras camadas\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Definindo o tamanho da entrada que vem do BERT\n",
    "        self.concat_last_n_bert_layers = concat_last_n_bert_layers\n",
    "        bert_output_dim = self.bert.config.hidden_size * concat_last_n_bert_layers\n",
    "\n",
    "        # 2. LSTM (Opcional)\n",
    "        if self.use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=bert_output_dim,\n",
    "                hidden_size=lstm_hidden // 2,\n",
    "                num_layers=1,\n",
    "                bidirectional=True,\n",
    "                batch_first=True\n",
    "            )\n",
    "            # A saída da Bi-LSTM será (hidden // 2) * 2 = lstm_hidden\n",
    "            self._init_lstm_weights()\n",
    "            classifier_input_dim = lstm_hidden\n",
    "        else:\n",
    "            # Se não tiver LSTM, o classificador recebe direto do BERT\n",
    "            classifier_input_dim = bert_output_dim\n",
    "\n",
    "        # 3. Classificador Final (Projeta para o número de labels)\n",
    "        self.classifier = nn.Linear(classifier_input_dim, num_labels)\n",
    "        \n",
    "        # Inicialização de pesos do classificador\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "        # 4. CRF (Opcional)\n",
    "        if self.use_crf:\n",
    "            self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def _init_lstm_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "                # Forget gate bias = 1\n",
    "                n = param.size(0)\n",
    "                param.data[n//4:n//2].fill_(1.0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Concatenar as últimas N camadas do BERT conforme artigo original\n",
    "        hidden_states = outputs.hidden_states\n",
    "        token_repr = torch.cat(hidden_states[-self.concat_last_n_bert_layers:], dim=-1)\n",
    "\n",
    "        x = self.bert_dropout(token_repr)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        emissions = self.classifier(x)\n",
    "        return emissions\n",
    "\n",
    "    def compute_loss(self, emissions, labels, attention_mask):\n",
    "        if self.use_crf:\n",
    "            safe_labels = labels.clone()\n",
    "            safe_labels[safe_labels == -100] = 0\n",
    "\n",
    "            loss = -self.crf(\n",
    "                emissions,\n",
    "                safe_labels,\n",
    "                mask=attention_mask.bool(),\n",
    "                reduction='mean'\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        return loss_fct(\n",
    "            emissions.view(-1, self.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def decode(self, input_ids, attention_mask):\n",
    "        if self.use_crf:\n",
    "            emissions = self.forward(input_ids, attention_mask)\n",
    "\n",
    "            predictions = self.crf.decode(\n",
    "                emissions,\n",
    "                mask=attention_mask.bool()\n",
    "            )\n",
    "            return predictions\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        return predictions.detach().cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b88c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd25a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID2LABEL = {\n",
    "    0: \"O\",\n",
    "    1: \"B-POS\",\n",
    "    2: \"I-POS\",\n",
    "    3: \"B-NEG\",\n",
    "    4: \"I-NEG\",\n",
    "    5: \"B-NEU\",\n",
    "    6: \"I-NEU\"\n",
    "}\n",
    "\n",
    "LABEL2ID = {\n",
    "    \"O\": 0,\n",
    "    \"B-POS\": 1,\n",
    "    \"I-POS\": 2,\n",
    "    \"B-NEG\": 3,\n",
    "    \"I-NEG\": 4,\n",
    "    \"B-NEU\": 5,\n",
    "    \"I-NEU\": 6\n",
    "}\n",
    "\n",
    "PAD_LABEL_ID = -100\n",
    "\n",
    "RELEVANT_LABELS = [\"B-POS\", \"I-POS\", \"B-NEG\", \"I-NEG\", \"B-NEU\", \"I-NEU\", \"POS\", \"NEG\", \"NEU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70ec4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este é um JSON criado manualmente com base na HOntology (que é a origem dos aspectos do dataset)\n",
    "# Ele serve para \"normalizar\" os aspectos\n",
    "# Ex: 'custo-benefício': ['custo-beneficio', 'custo beneficio', 'custo benefício']\n",
    "# Como os textos do dataset possuem variações de escrita dos aspectos, esse dicionário ajuda a mapear todas para a classe esperada\n",
    "with open('data/aspectos_use.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    replace_dict = {k: v for d in data for k, v in d.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "79db954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_reviews(df):\n",
    "    df[\"review_id\"] = df.groupby(\"review\").ngroup().apply(lambda x: f\"R{x+1}\")\n",
    "    grouped = defaultdict(lambda: {\"review\": \"\", \"aspects\": []})\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        rid = row[\"review_id\"]\n",
    "        grouped[rid][\"review\"] = row[\"review\"]\n",
    "        grouped[rid][\"aspects\"].append({\n",
    "            \"start\": row[\"start_position\"],\n",
    "            \"end\": row[\"end_position\"],\n",
    "            \"polarity\": row[\"polarity\"],\n",
    "            \"term\": row[\"aspect\"],\n",
    "        })\n",
    "\n",
    "    return list(grouped.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0276fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bio_tags_for_bert(item: dict) -> dict:\n",
    "    if not tokenizer:\n",
    "        raise ImportError(\"O tokenizador do BERTimbau não foi carregado.\")\n",
    "\n",
    "    text = item['review']\n",
    "    entities = item['aspects']  # Lista de entidades no formato {'start', 'end', 'polarity'}\n",
    "\n",
    "    # Tokeniza o texto e obtém os mapeamentos de offset\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=MAX_LENGTH, padding='max_length').to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    # word_ids = encoding.word_ids()\n",
    "\n",
    "    polarity_map = {-1: \"NEG\", 1: \"POS\", 0: \"NEU\"}\n",
    "    tags = [LABEL2ID['O']] * len(tokens) # Inicializa com o ID para 'O'\n",
    "    \n",
    "    for entity in sorted(entities, key=lambda x: x['start']):\n",
    "        start_char = entity['start']\n",
    "        end_char = entity['end']\n",
    "        polarity_label = polarity_map.get(entity['polarity'], \"UNK\")\n",
    "\n",
    "        is_first_token_in_entity = True\n",
    "        for i, (start_offset, end_offset) in enumerate(offset_mapping):\n",
    "            # Ignora tokens especiais como [CLS] e [SEP] que tem offset (0, 0)\n",
    "            if start_offset == end_offset:\n",
    "                tags[i] = PAD_LABEL_ID  # Marca tokens especiais com PAD_LABEL_ID (ignora no loss)\n",
    "                continue\n",
    "\n",
    "            # Se o span do subword cruza com o span da entidade\n",
    "            if max(start_offset, start_char) < min(end_offset, end_char):\n",
    "                if is_first_token_in_entity:\n",
    "                    tag_string = f\"B-{polarity_label}\"\n",
    "                    is_first_token_in_entity = False\n",
    "                else:\n",
    "                    tag_string = f\"I-{polarity_label}\"\n",
    "                tags[i] = LABEL2ID.get(tag_string, LABEL2ID['O'])\n",
    "\n",
    "    # Comentado a parte que atribui -100 para subwords (algo semelhante a uma lematização)\n",
    "    # previous_word_id = None\n",
    "    # for i in range(len(tags)):\n",
    "    #     if tags[i] == PAD_LABEL_ID:\n",
    "    #         continue\n",
    "    #     current_word_id = word_ids[i]\n",
    "    #     if current_word_id is None or current_word_id == previous_word_id:\n",
    "    #         tags[i] = PAD_LABEL_ID\n",
    "\n",
    "    #     previous_word_id = word_ids[i]\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": encoding['input_ids'],\n",
    "        \"labels\": tags,\n",
    "        \"attention_mask\": encoding['attention_mask'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a36316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc7b05a3ffd42b395c383a4874030c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('data/train2024.csv', sep=\";\", index_col=0)\n",
    "grouped_data = group_reviews(data)\n",
    "dataset = Dataset.from_list(grouped_data)\n",
    "dataset = dataset.map(create_bio_tags_for_bert)\n",
    "\n",
    "# Divisão entre treino, validação e teste\n",
    "# Essa divisão foi feita para testar diferentes abordagens de pré-processamento e arquiteturas\n",
    "# splits_1 = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = splits_1['train']\n",
    "# test_valid_dataset = splits_1['test']\n",
    "# splits_2 = test_valid_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# eval_dataset = splits_2['train']\n",
    "# test_dataset = splits_2['test']\n",
    "\n",
    "# # Verificando os tamanhos\n",
    "# print(f\"Treino: {len(train_dataset)}\")\n",
    "# print(f\"Validação: {len(eval_dataset)}\")\n",
    "# print(f\"Teste: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "103920a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(x[\"attention_mask\"], dtype=torch.long) for x in batch])\n",
    "    labels = torch.stack([torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "full_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# usados nos testes iniciais apenas\n",
    "# train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# eval_loader = DataLoader(eval_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "66fcbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, labels):\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [ID2LABEL[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ID2LABEL[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b53d4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "if RUN_CROSS_VALIDATION:\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f\"\\n========== Fold {fold+1}/{K} ==========\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset   = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        model = ABSA_LSTM_CRF(\n",
    "            bert_model_name=BERT_MODEL_PATH,\n",
    "            num_labels=len(ID2LABEL),\n",
    "            use_lstm=USE_LSTM,\n",
    "            use_crf=USE_CRF,\n",
    "            dropout=DROPOUT,\n",
    "            bert_dropout=BERT_DROPOUT,\n",
    "            lstm_hidden=LSTM_HIDDEN,\n",
    "            concat_last_n_bert_layers=CONCAT_LAST_N_BERT_LAYERS\n",
    "        ).to(device)\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=PAD_LABEL_ID)\n",
    "\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.bert.parameters(), \"lr\": 1e-5},\n",
    "            {\"params\": model.classifier.parameters(), \"lr\": 3e-4},\n",
    "        ])\n",
    "\n",
    "        if model.use_lstm:\n",
    "            optimizer = torch.optim.AdamW([\n",
    "                {\"params\": model.bert.parameters(), \"lr\": 1e-5},\n",
    "                {\"params\": model.lstm.parameters(), \"lr\": 1e-4},\n",
    "                {\"params\": model.classifier.parameters(), \"lr\": 3e-4},\n",
    "            ])\n",
    "\n",
    "        # ===== TREINO =====\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss = loss_fct(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Train loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # ===== AVALIAÇÃO (SEQEVAL) =====\n",
    "        model.eval()\n",
    "\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                logits = model(input_ids, attention_mask)\n",
    "\n",
    "                all_logits.append(logits.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_logits = np.concatenate(all_logits, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "        metrics = compute_metrics(all_logits, all_labels)\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold+1} | \"\n",
    "            f\"F1: {metrics['f1']:.4f} | \"\n",
    "            f\"P: {metrics['precision']:.4f} | \"\n",
    "            f\"R: {metrics['recall']:.4f}\"\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9833d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CROSS_VALIDATION:\n",
    "    kf_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "    # Adiciona coluna do fold\n",
    "    kf_df.insert(0, \"fold\", np.arange(1, len(kf_df) + 1))\n",
    "    kf_df.to_csv(\"kfold_seqeval_results.csv\", index=False)\n",
    "\n",
    "    print(\"\\nResultados salvos em kfold_seqeval_results.csv\")\n",
    "\n",
    "    print(f\"F1 médio: {kf_df['f1'].mean():.4f}\")\n",
    "    print(f\"Desvio padrão: {kf_df['f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "536cc48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\Franco\\Desktop\\projetos\\bert\\bertimbau_dapt_final_nongrouped and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train loss: 0.3907\n",
      "\n",
      "Epoch 2\n",
      "Train loss: 0.0656\n",
      "\n",
      "Epoch 3\n",
      "Train loss: 0.0509\n",
      "\n",
      "Epoch 4\n",
      "Train loss: 0.0417\n"
     ]
    }
   ],
   "source": [
    "# Treinamento Final\n",
    "model = ABSA_LSTM_CRF(\n",
    "    bert_model_name=BERT_MODEL_PATH,\n",
    "    num_labels=len(ID2LABEL),\n",
    "    use_lstm=USE_LSTM,\n",
    "    use_crf=USE_CRF,\n",
    "    dropout=DROPOUT,\n",
    "    bert_dropout=BERT_DROPOUT,\n",
    "    lstm_hidden=LSTM_HIDDEN,\n",
    "    concat_last_n_bert_layers=CONCAT_LAST_N_BERT_LAYERS\n",
    ").to(device)\n",
    "\n",
    "total_steps = len(full_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * 0.1) \n",
    "\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss(ignore_index=PAD_LABEL_ID) \n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.bert.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "\n",
    "if model.use_lstm:\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.bert.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.lstm.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 3e-4},\n",
    "    ])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # recomendado\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Train loss: {total_loss / len(full_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2ba5151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2058ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_term(term):\n",
    "    term = term.strip().replace(\" - \", \"-\")\n",
    "    term = term.lower()\n",
    "    for replace_to, replace_words in replace_dict.items():\n",
    "        if term in replace_words:\n",
    "            term = replace_to\n",
    "            # print(f\"Replaced '{term}' with '{replace_to}'\")\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2878dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_bi_tags_old(tag_list):\n",
    "    result = []\n",
    "    current_phrase = \"\"\n",
    "    current_tag = None\n",
    "    # previous_item = None\n",
    "    for item in tag_list:\n",
    "        if not isinstance(item, tuple) or len(item) != 2:\n",
    "            print(f\"Invalid item: {item}\")\n",
    "            continue  # Skip invalid entries\n",
    "\n",
    "        word, tag = item\n",
    "        if not isinstance(tag, str):\n",
    "            print(f\"Invalid tag type for item: {item}. Expected string, got {type(tag).__name__}.\")\n",
    "            # print(f\"Item: {item}\")\n",
    "            # print(f\"Previous item: {previous_item}\")\n",
    "            continue # Skip this item or handle it differently\n",
    "    \n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_phrase:\n",
    "                term = process_term(current_phrase)\n",
    "                result.append((term, current_tag))\n",
    "            current_phrase = word\n",
    "            current_tag = tag\n",
    "\n",
    "        elif tag.startswith(\"I-\") and current_tag and tag[2:] == current_tag[2:]:\n",
    "            current_phrase += f\" {word}\"\n",
    "        else:\n",
    "            if current_phrase:\n",
    "                term = process_term(current_phrase)\n",
    "                result.append((term, current_tag))\n",
    "            result.append((word, tag))\n",
    "            current_phrase = \"\"\n",
    "            current_tag = None\n",
    "        \n",
    "        # previous_item = item\n",
    "    if current_phrase:\n",
    "        term = process_term(current_phrase)\n",
    "        result.append((term, current_tag))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a8ff2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2POLARITY = {\n",
    "    \"POS\": 1,\n",
    "    \"NEG\": -1,\n",
    "    \"NEU\": 0,\n",
    "    \"B-POS\": 1,\n",
    "    \"I-POS\": 1,\n",
    "    \"B-NEG\": -1,\n",
    "    \"I-NEG\": -1,\n",
    "    \"B-NEU\": 0,\n",
    "    \"I-NEU\": 0,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f58413bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupa B's e I'\n",
    "# Caso haja conflito de tags dentro de uma mesma entidade (ex: B-NEU + I-NEG), resolve pegando a tag mais frequente\n",
    "# Remove tags \"órfãs\" (I- sem B- anterior)\n",
    "# Trasforma tags em polaridade\n",
    "def group_bi_tags(tag_list):\n",
    "    result = []\n",
    "    current_phrase = [] \n",
    "    current_tags_seen = [] \n",
    "    \n",
    "    for item in tag_list:\n",
    "        if not isinstance(item, tuple) or len(item) != 2: \n",
    "            continue\n",
    "        word, tag = item\n",
    "        \n",
    "        # --- CASO 1: Início de Entidade (B-) ---\n",
    "        if tag.startswith(\"B-\"):\n",
    "            # 1.a) Se já tinha algo aberto, fecha e salva\n",
    "            if current_phrase:\n",
    "                term = \" \".join(current_phrase)\n",
    "                # Resolve conflito (ex: B-NEU + I-NEG vira NEG)\n",
    "                final_tag = Counter(current_tags_seen).most_common(1)[0][0]\n",
    "                result.append((term, final_tag))\n",
    "            \n",
    "            # 1.b) Abre nova entidade\n",
    "            current_phrase = [word]\n",
    "            current_tags_seen = [tag[2:]] # Guarda 'NEU' sem o B-\n",
    "\n",
    "        # --- CASO 2: Continuação Válida (I-) ---\n",
    "        # Só entra aqui se for I- E se tivermos uma frase aberta\n",
    "        elif tag.startswith(\"I-\") and current_phrase:\n",
    "            current_phrase.append(word)\n",
    "            current_tags_seen.append(tag[2:]) # Guarda 'NEG' sem o I-\n",
    "\n",
    "        # --- CASO 3: Resto (O, ou I- Órfão) ---\n",
    "        else:\n",
    "            # 3.a) Se tinha entidade aberta, fecha e salva ela antes de processar o atual\n",
    "            if current_phrase:\n",
    "                term = \" \".join(current_phrase)\n",
    "                term = process_term(term)\n",
    "                final_tag = Counter(current_tags_seen).most_common(1)[0][0]\n",
    "                result.append((term, final_tag))\n",
    "                current_phrase = []\n",
    "                current_tags_seen = []\n",
    "\n",
    "            # 3.b) Processa a palavra atual\n",
    "            # Se for órfão (I- perdido), forçamos virar 'O'. Se já for 'O', mantém.\n",
    "            if tag.startswith(\"I-\"):\n",
    "                tag_corrigida = 'O' \n",
    "            else:\n",
    "                tag_corrigida = tag\n",
    "            \n",
    "            # ADICIONA A PALAVRA ATUAL À LISTA (Isso evita ficar vazio)\n",
    "            result.append((word, tag_corrigida))\n",
    "\n",
    "    if current_phrase:\n",
    "        term = \" \".join(current_phrase)\n",
    "        term = process_term(term)\n",
    "        final_tag = Counter(current_tags_seen).most_common(1)[0][0]\n",
    "        result.append((term, final_tag))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a496208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSA_Predictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        bert_model_name='neuralmind/bert-base-portuguese-cased',\n",
    "        max_len=512,\n",
    "        device='cuda',\n",
    "        use_lstm=False,\n",
    "        use_crf=False,\n",
    "        lstm_hidden=256,\n",
    "        dropout=0.3,\n",
    "        bert_dropout=0.1,\n",
    "        concat_last_n_bert_layers=4\n",
    "    ):\n",
    "        self.max_len = max_len\n",
    "        self.use_crf = use_crf\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "        self.model = ABSA_LSTM_CRF(\n",
    "            bert_model_name=bert_model_name,\n",
    "            num_labels=len(ID2LABEL),\n",
    "            use_lstm=use_lstm,\n",
    "            use_crf=use_crf,\n",
    "            lstm_hidden=lstm_hidden,\n",
    "            dropout=dropout,\n",
    "            bert_dropout=bert_dropout,\n",
    "            concat_last_n_bert_layers=concat_last_n_bert_layers\n",
    "        )\n",
    "\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print(\"Modelo carregado\")\n",
    "\n",
    "    def predict(self, text):\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emissions = self.model(input_ids, attention_mask)\n",
    "\n",
    "            if self.use_crf:\n",
    "                predictions = self.model.crf.decode(\n",
    "                    emissions,\n",
    "                    mask=attention_mask.bool()\n",
    "                )[0]  # batch = 1\n",
    "            else:\n",
    "                predictions = torch.argmax(emissions, dim=-1)[0].tolist()\n",
    "\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "        result = []\n",
    "        for i, tag_id in enumerate(predictions):\n",
    "            if attention_mask[0, i] == 0:\n",
    "                continue\n",
    "\n",
    "            token = tokens[i]\n",
    "            if token in ['[CLS]', '[SEP]']:\n",
    "                continue\n",
    "\n",
    "            label = ID2LABEL[tag_id]\n",
    "\n",
    "            if token.startswith(\"##\") and result:\n",
    "                last_word, last_tag = result[-1]\n",
    "                result[-1] = (last_word + token[2:], last_tag)\n",
    "            else:\n",
    "                result.append((token, label))\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cbcd4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\Franco\\Desktop\\projetos\\bert\\bertimbau_dapt_final_nongrouped and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado\n"
     ]
    }
   ],
   "source": [
    "predictor = ABSA_Predictor(\n",
    "    model_path=MODEL_NAME, \n",
    "    bert_model_name=BERT_MODEL_PATH,\n",
    "    max_len=MAX_LENGTH,\n",
    "    use_lstm=USE_LSTM,\n",
    "    use_crf=USE_CRF,\n",
    "    lstm_hidden=LSTM_HIDDEN,\n",
    "    dropout=DROPOUT,\n",
    "    bert_dropout=BERT_DROPOUT,\n",
    "    concat_last_n_bert_layers=CONCAT_LAST_N_BERT_LAYERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a865c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(csv_path, predictor, output_file=\"absa_results.csv\", use_old_grouping=False, save_full_pred=False):    \n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "    print(f\"Avaliando {len(df)} reviews...\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row['review']\n",
    "        raw_pred = predictor.predict(text)\n",
    "        \n",
    "        gt = ast.literal_eval(row['aspects'])\n",
    "        if use_old_grouping:\n",
    "            grouped_pred = group_bi_tags_old(raw_pred)\n",
    "        else:\n",
    "            grouped_pred = group_bi_tags(raw_pred)\n",
    "\n",
    "        relevant = [item for item in grouped_pred if item[1] in RELEVANT_LABELS]\n",
    "        relevant = [(term, LABEL2POLARITY[tag]) for term, tag in relevant]\n",
    "\n",
    "        result = {\n",
    "            \"preds\": relevant,\n",
    "            \"labels\": gt,\n",
    "            \"review\": text,\n",
    "        }\n",
    "\n",
    "        if save_full_pred:\n",
    "            result[\"full_pred\"] = grouped_pred\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    save_to = output_file\n",
    "    if use_old_grouping:\n",
    "        save_to = output_file.replace(\".csv\", \"_oldgrouping.csv\")\n",
    "    pd.DataFrame(results).to_csv(save_to, sep=';', index=False)\n",
    "    print(f\"Resultados salvos em '{save_to}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e76f5a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando 282 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:20<00:00, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'absa_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# imagino que seria melhor fazer essa transformação no mesmo arquivo pra ficar mais fácil de comparar\n",
    "dataset_teste = \"data/task2_full_test_grouped.csv\"\n",
    "\n",
    "generate_results(dataset_teste, predictor)\n",
    "# generate_results(dataset_teste, predictor, use_old_grouping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4877835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação no dataset de teste da task 2 para testar Extração de Aspectos + Classificação de Sentimento\n",
    "def evaluate_task2(df_name):\n",
    "    df = pd.read_csv(f\"{df_name}\", sep=\";\")\n",
    "\n",
    "    df['preds'] = df['preds'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['labels'] = df['labels'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    def key_of(x):\n",
    "        if isinstance(x, (list, tuple)) and len(x) > 0:\n",
    "            return (x[0], x[1])\n",
    "        return x\n",
    "\n",
    "    def to_multiset(seq):\n",
    "        c = Counter()\n",
    "        if seq is None or (isinstance(seq, float) and math.isnan(seq)):\n",
    "            return c\n",
    "        for x in seq:\n",
    "            if x is None or (isinstance(x, float) and math.isnan(x)):\n",
    "                continue\n",
    "            k = key_of(x)\n",
    "            if k is None:\n",
    "                continue\n",
    "            c[str(k).strip().lower()] += 1\n",
    "        return c\n",
    "\n",
    "    tp = fp = fn = 0\n",
    "    total_gold = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        gold = to_multiset(row[\"labels\"])\n",
    "        pred = to_multiset(row[\"preds\"])\n",
    "\n",
    "        row_tp = sum(min(gold[e], pred[e]) for e in gold)\n",
    "        row_fp = sum(max(pred[e] - gold.get(e, 0), 0) for e in pred)\n",
    "        row_fn = sum(max(gold[e] - pred.get(e, 0), 0) for e in gold)\n",
    "\n",
    "        tp += row_tp\n",
    "        fp += row_fp\n",
    "        fn += row_fn\n",
    "        total_gold += sum(gold.values())\n",
    "        total_pred += sum(pred.values())\n",
    "\n",
    "    # Micro (sobre todos os itens)\n",
    "    micro_p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    micro_r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r) if (micro_p+micro_r) > 0 else 0.0\n",
    "\n",
    "    print(f\"TP: {tp} / itens-ouro: {total_gold}\")\n",
    "    print(f\"FP: {fp} / itens-previstos: {total_pred}\")\n",
    "    print(f\"FN: {fn}\")\n",
    "    print(f\"[micro]  f1: {micro_f1:.3f} precisão: {micro_p:.3f}  recall: {micro_r:.3f}  \")\n",
    "    print(\"===================\")\n",
    "\n",
    "    return micro_f1, micro_p, micro_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9418596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 850 / itens-ouro: 1176\n",
      "FP: 316 / itens-previstos: 1166\n",
      "FN: 326\n",
      "[micro]  f1: 0.726 precisão: 0.729  recall: 0.723  \n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "f1, p, r = evaluate_task2(\"absa_results.csv\")\n",
    "\n",
    "with open(\"final_results_task2.txt\", \"a\") as f:\n",
    "    f.write(f\"{MODEL_NAME}\\t\")\n",
    "    f.write(f\"F1: {f1:.4f}\\t\")\n",
    "    f.write(f\"P: {p:.4f}\\t\")\n",
    "    f.write(f\"R: {r:.4f}\\n\")\n",
    "# evaluate_task2(\"absa_results_oldgrouping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1988237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
